SPEAKER 0
My son. Has one for me. I.

UNKNOWN
Keep on saying. I don't know, because there.

SPEAKER 0
Are always awards.

UNKNOWN
For the little goody goody one of the camps. Thank.

SPEAKER 0
You all for delivering to the 7 p.m. so I can't complain. But you will have to ponder. I'm.

UNKNOWN
Talking to an 1150 year old.

SPEAKER 0
Oh, that's why you do it now. Oh, yeah. Oh, yeah. No, no. No. No. You don't know what was up. Rubio making that up. Michael unstoppable in the. Now. There are ten transactions. Hundred dollars. In. Debit card transactions. Not or you know that is a mobile bank $60.

UNKNOWN
You know everything.

SPEAKER 0
I love. But there was a Thursday morning. Another one on the road. One time. You know, trying. And trying hard for money for some beer and stuff. But it's not good. It's not good at home.

UNKNOWN
300 max is all over the place.

SPEAKER 0
I mean. I know this is such a big picture. Come on.

UNKNOWN
Man. Okay. I.

SPEAKER 0
Think we're getting close. I think we're.

SPEAKER 1
Done take.

SPEAKER 0
A.

SPEAKER 1
Look.

SPEAKER 0
So you want to go to the. Countryside to. Do that. I got your phone call. Last night. Hello. You know, uh, Olivia.

UNKNOWN
Hello? Hello. Uh, you can do this. Uh.

SPEAKER 0
What do you get there? I. Don't know. You know, but. Yeah. You live here? Oh. I know, I know, but on the professor actually, you know, for. Our class now at 530. We have class for some reason, but. Yeah, for 650, 650, 7:00. I'm not concerned.

UNKNOWN
Okay. Let's. Take a look at. Okay. You. Know? What?

SPEAKER 0
We already have a canceling gym. You're going good, Serena.

UNKNOWN
Okay. Thank you. One more.

SPEAKER 0
One more. Where do you think? See, I was. Thinking like a shoot out. Like, you know, some my brother and brother and.

UNKNOWN
All the rest of us. Are important.

SPEAKER 2
Everyone okay?

SPEAKER 3
All right.

SPEAKER 2
Now, before we get started at this point, everybody that just took to the class, even if they are a late registrant or a swap or anything like that, should be in their appropriate, uh, section of uh, on canvas site, should be on that appropriate section on canvas. So if you are registered to this particular section and are not in this section's canvas page, please let me know or more importantly, let you know as soon as possible and get it fixed. Right now, if you are on the appropriate canvas page for your course, then you should have no limitations in making your submission for your policy acknowledgment form. Right. I hope everybody has already made their policy acknowledgment form. If you haven't done so yet, now's your chance. The deadline is September 4th. If you do not make your submission, then you will not be allowed to attend any of the, uh, exams for the course, which will result in you getting an automatic F, so there really is no reason to wait till the last second.

UNKNOWN
All right. Okay.

SPEAKER 2
Now, last class we were looking at how to go about building goal based agents. Right. Idea is if for any problem, like, say, for example, I gave you a graph plane problem and you wanted to solve it using state space search. Sorry. If you wanted to build a goal based agent to solve it, well, first thing you do is define the single state problem formulation, right? For any problem, first step in building a goal based agent is to define the single state problem formulation, which consists of, uh, state initial state description, a successor function, a function which, given any state description, will return the set of all actions that can be performed in that state, and the outcome of those actions in terms of state descriptions. Write a step cost function a function which, given any action that can be performed in that domain, will return an additive numerical cost, preferably positive real numbers. And finally, a gold test, a test which will take a state description and tell you whether or not you are done. If you are in a state that matches that description right now, this gold test can be explicit, in which case there is only one state that satisfies that condition, and if you reach that state, you are done. Or it could be implicit. There are any number of states that can satisfy this conditions. And if you reach any one of them, you are done right. Now note while I am going to be using a graph navigation problem to illustrate how this algorithm works, it is not the only type of problem for which you can build a gold based agent, Like say for example, this is the single state problem formulation for a 8% problem for a robot arm manipulation problem. So you can come up with a single state problem formulation for a wide variety of problem, not all for a wide variety of problem, which means that you can build a goal based agent for all of those problems. Now in later class, we are going to look at uh, approaches for building, uh, goal based agents that require certain conditions, like it require your state to be defined a certain way or your successor function to work a certain way, etc., etc. but what we are going to look at, what we are in the process of looking at, is the generic approach. As long as you have the single state problem formulation, you can, in theory, use this approach to build an agent for solving that problem, even though it's not necessarily going to be the most efficient way to go about solving that problem, right? It is just the option that's always available, and the idea behind it is very straightforward. It's a simulated exploration of your state space. So you will take your initial state according to your successor function. What are the states that you can reach from those states? What are the states you can reach from those states? What are the states you can reach? And you will keep doing this until you reach a state that satisfies your goal condition. Right. And whichever state satisfies sorry. And once you find a state that satisfies your goal condition, all you need to do is return the sequence of actions that will get you there from your initial state. So it is offline. Problem solving. Right now this approach is called tree search, not because you are actually building a tree and searching to it, but simply because of the behavior exhibited by your algorithm. The implementation of the algorithm is fairly straightforward. You have your fringe, which is the list of candidates of uh, x, sorry list of candidates for exploration, which contains search nodes. In each loop of your search, you will check is the fringe empty? If it is, your search failed, no solution possible. Otherwise you will pop a search node from your fringe. You will check is the state of that search node satisfying my goal condition? If it does, you are done. Otherwise, uh, you will generate the successors of that particular search node and add them to the fringe. Right now. If it does satisfy the goal condition, then you will use that additional information. As we discussed last class that you include in the node description to reconstruct the sequence of actions you need to take to get to that particular node. And that is your solution right now you can either do tree search or you can do graph search. Now both of them. The only difference is whether are you or are you not worried about repeated states, but otherwise they work exactly the same. Now, the reason it's called research is simply because of the behavior exhibited by your algorithm, which is a simulated exploration of the hypothetical search tree. Like, let's say I take a very simple graph navigation problem similar to the example we saw last class. Right. So let's say I'm starting at a and I'm going to get to goal the. Now note you won't be given a problem like this. you in practice would be given a single state problem formulation, right? You will be given an initial state. You will be given a gold test in the initial state in a successor function, a function which will take any given state in this domain and return what actions can be performed in their outcome. For example, for C it will return. I can go from C to be. I can go from C to D result of first action. I will be in B result of second action I will be in D, right, and a step cost function a function which for any action that can take place in this domain return an additive numerical cost. Cost of taking action B to a six. Right? Something like that. So that single state problem formulation is what you will be given right now if you do tree search or graph search. What is actually happening is you will just keep adding removing search nodes from your fridge. But the behavior exhibited by your our algorithm is a simulated traversal of this tree. Etc. etc., etc. stretching out to infinity. Right. So this hypothetical tree is the tree that your algorithm effectively traverses to try and find a sequence of actions. That will get it from the start state to a state that satisfies the goal condition. Right. Now. When you are implementing either tree search or graph search, what do you need to pay attention to is how exactly you are implementing this one line of pseudocode. That is, what is the order in which you insert new candidates into your fridge, because that determines order of node expansion and that determines whether if a solution exists, are you guaranteed to find it? If multiple solutions exist, will you find the one with the lowest cumulative cost to find a solution. How many nodes will you end up generating? And of those nodes, how many of them need to stay in memory until you find a solution? All of that are determined by that by your choice for this one line in the pseudocode. What data structure you use? How do you insert them? How do you pop right now a small node? Regarding time and space complexity, they are usually represented in terms of factors like branching factor or depth or max possible depth. These are terms that correspond to the hypothetical search tree. Right. So when I say branching factor I mean what is the branching factor of this tree. But what it actually translates to is what is the maximum number of actions that can be taken from any given state. Right. Oh, by the way, uh, some literature, not all, and some literature, especially ones you find online, will do something odd where they'll average the number of actions, which makes no mathematical sense, by the way. So if, let's say I have a problem from some states, I can take two actions from other states I can take five actions. What's the branching factor? And know. What is the maximum number of actions I can take in any given state? Five. So that's the branching factor you use because that allows you to do worst case analysis. Some literature will say average it, which in this case would mean 3.5 actions, which makes no sense whatsoever. Right. And again when they say depth of shallower solution, what they actually mean is what is the what is the minimum number of actions required to reach a solution. Right. That is what depth of shallow a solution resolution means because that translates to at what depth does a solution first appear in your hypothetical search tree? And finally, miss, how big can your hypothetical search tree get, which again, depending on your problem, can be infinity? Right. So the first set of search strategies we are going to look at are what are called uninformed search strategies. Now they are called that because you do not need to do any analysis to determine order of node expansion expansion. It is taken care of either by how you implemented the algorithm, or some piece of information that was provided as part of your problem description. You do not have to come up with some ranking system for the search nodes. If you did, that would be what is called inform search, which is we are going to look at that also in a little bit. Right. These are strategies where the order of node expansion is determined simply by how you implemented the code, or by information that is part of your single state problem formulation. Now, some of these might be terms that you have come across previously. Again, you're not actually going to do breadth first traversal on a tree, right. It's just that if you are trying to do tree search with breadth first search strategy, you will implement this pseudocode with that highlight line implemented in such a way that the behavior exhibited by your algorithm is a breadth first traversal of the hypothetical search tree. Right. So let's look at these search strategies. So let's say I gave you some arbitrary tree and I asked you to do breadth first traversal on it. What would you do. In what order will you visit nodes if you are doing a breadth first traversal on a tree. Left and right don't really make sense, right? There is nothing preventing you from going right to left. Right. It is shallowest node first right. The node that is at the lowest depth in the tree first right. So to get that same behavior here, all you have to do is implement your fringe as a first in first out data structure like a queue. Right. So if you implement your fringe as a queue you will get that behavior. So let's say I have this problem I implemented my fringe as a data structure where candidates for expansion are removed from this end, but added in this end. Right. So. Initially my fringe has the node a I will pop that node is a my goal. No, it is not. So I'll generate it. Successes. Add them to the French. Now the only restriction is they have to be added to whatever is already there in the fridge. If there is anything in the French. But other than that there is no restriction. So I could add b followed by e or E followed by B. It's perfectly fine, right? So let us say I added B followed by e. Pop. The node from the front of the fridge is B my goal? No it is not. So generate its successors. Pop. The node at the front of the fridge is E my goal? No it is not. So generate its successors. For the node at the front of the fringe is a my goal. No, it is not. So generate its successors. Note if I was doing graph search, I would not have expanded this. I would not have generated these successors because when I pop this node, I would have noticed that the state is already enclosed. So I would have just said, I'm not going to bother expanding it again and not bothered to expand it. But here, since I am doing tree search, I would have generated these successors and added them to the fringe. Pop. The next node is see my goal? No, it is not. So generate its successors. While the next node is a my goal node is not, so generate its Successes. But the next note is D my goal. Yes it is. I got this note from this note, which I got to from this note. So my solution is. Which I can reconstruct. Like how I showed you last class. Right. Because the node architecture will have the action that I took to get here. So I can just reconstruct the path from that. Right. Now is this complete? That is, if a solution exists, are you guaranteed to find it, or is there a scenario where you could possibly not find a solution even though one exists? Everyone. Y'all are grad students, okay? How many actions are possible from a given state here? What's the branching factor? Six. Why six?

SPEAKER 4
No they're not.

SPEAKER 2
There are six joint angles. That doesn't mean there are only six actions possible. How many actions are possible? I'm sorry. Why? By how much? There are an infinite number of possible actions in theory. Right. It is continuous changes to the giant angle. I can change it by any amount. A continuous value, which means that there are two pi possible actions per joint angle. Well, there are two pi degree radians and I can change it by any radian amount. Right. So technically infinite number of possible actions in theory but in practice not really. I can only change the joint angle by some resolution. I can change it by, I don't know, tenths of a degree or one hundredths of a degree or thousands of a degree maybe. So while B is going to be really, really, really large, it's not infinite, right? So for all practical purposes, BFS is finite, but technically it is only finite as long as B is finite. Right. So most literature will just say it is complete. But you need to be aware it's only complete as long as B is finite. If there is some scenario where it is possible for you to have an infinite B, then BFS is technically not finite, right? But for most scenarios B will have to be finite, either due to practical limitations or because of the nature of the problem right now. In the worst case scenario, how many nodes would you need to generate before you find your solution? Let us say my solution occurs at level D, right? It will generate and expand every node on level zero, level one, level two all the way till level D. It will generate but not expand most of the nodes on level D plus one, right? So if measured in terms of nodes generated, the time complexity is Big-O of B to the power of D plus one, Right. Which is exponential two d right. So that's the time complexity. By the way, if you instead measured it in terms of how many nodes are actually expanded, it would be B to the power of D, right. It's usually measured in terms of nodes generated because those nodes are generated and added to the fringe. It's just that you won't ever bother expanding them right now of these nodes. All the nodes that I generated. Let's say for example I consider this node. Is there any point during the search till I actually reached a solution where I could go, I don't really need this node so I can discard it and recover that memory. Yes. No. When? I said, before you get to your solution at any point before you get to your solution. Once you reach a solution, you are done. Everything can go out of the memory once you reach a solution.

SPEAKER 0
Even before a solution.

SPEAKER 2
When we expanded both nodes children, nodes, we can discard the old one, right? Why? How else are you going to reconstruct the path? Once you're done.

UNKNOWN
You can use a copy.

SPEAKER 2
So you are just going to change the overhead from this to something else. Okay. How are you going to do that for the eight puzzle problem. What is the graph for the eight puzzle problem? I keep telling you guys, this is not just for the graph navigation problem, right? This is for any problem for the robot arm manipulation problem. What's the graph look like? You cannot. Every node Renault generator has to be stored in memory till your search finishes, because you do not know till your search finishes whether or not it might be on the path to the goal. Right. So the space complexity of breadth first search is also B to the power of D plus one. And finally, is it optimal? Well, the only guarantee with breadth first search is you will find the shallowest solution. Or you will find the solution that requires the fewest number of actions. That is not necessarily going to be the optimal solution. Like for example over here. It is not the optimal solution, right? This solution has a cumulative cost of 18. There are solutions that cost less than that right. So in general it is not considered to be optimal. But if you are dealing with a problem where the shallower solution is guaranteed to be the optimal solution. Like the eight puzzle problem or the chess playing problem. Those are all problems where the best solution is the solution that reaches the goal state in the fewest number of actions. Right. So those are problems where BFS is going to be optimal. Now one bit of confusion. Your textbook lists this as oh, if your problem has one per action as your step cost function, that's not necessarily the restricting factor. I could have two per action for every action, and that BFS will still find the optimal solution. If you really wanted to, you can write it as if every action costs exactly the same. But a better way to put it would be if your problem is one where the shallowest solution is guaranteed to be the optimal solution. That is, the optimal solution is a solution that takes the fewest number of actions. Then BFS is optimal, But in general it's not. So that's one issue, the fact that it's not optimal in general. The other issue is the space complexity. B to the power of D plus one can become very large number very very quickly. Right. Say for example for the 8% problem what is the branching factor. What is the maximum number of actions I can take from a given state in the eight puzzle problem? I can't hear you. For four. Right. So and usually a eight puzzle problem a solvable eight puzzle problem. Because not all eight puzzle problems are solvable. Solvable eight puzzle problems tend to have solutions like, let's say 1520 steps. So let's say I have a eight puzzle problem that required 20 steps to solve. I would need to explore and store. Sorry, I would need to generate and store four to the power of 21 nodes in memory before I find that solution, which is significantly large number, right? So space complexity can become really bad for BFS. But before we get to that, let's look at the other problem the fact that it is not optimal in general when you are doing tree traversal. How did you solve this problem? Like if I am trying to find the vertex that is cheapest to get to, right? So to find the vertex that takes the fewest number of hops, I would use breadth first traversal for the vertex that is the cheapest to get to. What do I do?

SPEAKER 0
Calculate the least cost.

SPEAKER 2
Yeah, you will do cheapest first, cheapest instead of shallows. First you will do cheapest first. And to get that behavior from our search, all you have to do is instead of implementing your fringe as a queue, you will instead implement it as a priority queue, right where the priority is determined by the cumulative cost of getting to the node. The lower the cumulative cost of getting to a search node is, the further ahead in the fringe it's going to be. So you will implement your fringe as a sorted queue where the criteria for sorting is the g of n value. By the way, g of n is the term used to represent the cumulative cost of getting to a node. It represents. The sum of the step cost function for all the actions I had to take to get to that particular n, is it g of and value right? So initially my fringe, which now, by the way, is implemented as a priority queue where the priority for sorting is the g of n value Initially it going to contain, it is going to contain a node that represents the initial state. I have also written the GF and value there for clarity sake, right. So I will pop this node is a my goal. No it is not. So I will generate its successors, which is going to be B with a cumulative cost of six and E with a cumulative cost of eight. Those will get inserted into my priority queue. Now, there are a number of ways in which you could implement your priority queue. When you are doing it in code, you could. There are data structures that will sort entries as and when you insert them. Or you could just have a queue, insert them and then run a sorting algorithm on them. Whichever way is more optimal for your particular hardware or programing language, you will use that. The key thing is your queue will be a data structure where once the insertion process is finished, the nodes will be arranged in increasing order of g of n, and every time you pop a node, you will pop the node with the lowest g of n first. So the next node that will get popped is this one right? So is B my goal? No, it is not. So I will generate its successors, which is going to be. And uh oh sorry. So those get added here. Pop the next node is E my goal. No it is not. So generate its successors. You will notice I generated the suboptimal goal. But because its g of n is so high, it's gotten pushed to the back of the French pop the next node. Uh. Let's see. Uh, you see my goal? No, it is not so. Bob. The next node. Oh, by the way, when I popped the next node, I could have popped either one of these two nodes. If there are multiple nodes with the lowest g of n, I can pop any one of them. So let's say I popped this one first is a my goal. No, it is not. So generate its successors. Pop the next node, this node, which is the goal condition. So the solution I end up generating is. Which has a cumulative cost of 12 and is thus the optimal solution. Right. So this is uniform cost. Well this is tree search with uniform cost search strategy. If I had instead done graph search I would have maintained a closed set of states. And I wouldn't have bothered expanding this node, saving myself a little bit of work. Right. Now, is this complete? If a solution exists, are you guaranteed to find it? Well, consider this scenario. Initially, my friends will now have a with a cumulative cost of zero. I will expand that node, generate its successors, which is going to be B with a cumulative cost of six, and E with a cumulative cost of eight. I will expand B next, whose successors are going to be A with a cumulative cost of 12, C with a cumulative cost of ten, and E with a cumulative cost of six. So the next node to be expanded will be that node whose successors are going to be a with a cumulative cost of 14 D, with a cumulative cost of 16, and B with a cumulative cost of six, which will be the next node expanded. And you see what this is going right now. As long as you can guarantee that every action has a non-zero step cost, no matter how small, then it is complete, but In other words, it's not complete. Now for more problems. This is very easy to enforce, right? If you are not sure, just add a small offset to every action step cost right. And that will ensure that you will never have a zero step cost. Now what is the time complexity like now? It will expand every node whose g often is lower than the cost of the optimal solution, and generate their successors and add them to the fringe. It may end up expanding every node whose g of n is the same as the cost of the optimal solution, and when it does expand them, it will add their successors to the fringe. It will never expand any node whose g of n is greater than the cost of the optimal solution. So the time complexity is exponential to the cost of the optimal solution, But technically it is exponential to the cost of the optimal solution divided by the lowest possible step cost, right? Similar to BFS. Every node generator has to be stored in memory till the search finishes. In fact, BFS is considered to be a special case of the uniform cost search versus UX, where the cost of its optimal solution is dependent on the depth, right? And finally, since nodes are expanded in increasing order of g of m, it means that even if you do generate a suboptimal goal, it will keep getting pushed to the back of the fringe, because you will keep expanding the nodes whose g of n is smaller than that, which means at some point you will end up generating and then expanding the optimal goal before you get a chance to expand the suboptimal code. So the first time you expand a node that satisfies the goal condition is also the cheapest way to get to that code. So uniform cost search is guaranteed to give you the optimal solution. So you have these two methods now. So if you are dealing with a problem where the shallower solution is optimal right. You can use BFS. And otherwise you can use UX. And both of them are complete right. B is going to be finite due to practical limitations. And making sure that every action has a non-zero step. Cost is as simple as adding an offset to every action step cost, right? So every actions cost add 0.001. Then I made sure that I don't have zero cost actions right? So what's the catch? Space complexity. For both these approaches, every node that you generate has to be stored in memory, which means that it can get extremely memory intensive very quickly. So that brings us to depth first traversal. Now, if I gave you a tree and ask you to do depth first traversal on it, what are you supposed to do? In what order would you visit nodes for BFS? Shallowest first for UX, cheapest first for DFS.

SPEAKER 5
It's a mini.

SPEAKER 0
DFS first right.

SPEAKER 2
So you will explore the tree deepest node first right. So that is depth first search strategy sorry depth first traversal. And to get the same behavior with a tree search you will implement your fringe as a last in, first out data structure.

UNKNOWN
A stack, right.

SPEAKER 2
So I am not going to illustrate it with this problem. First we are going to use the example that your textbook uses. So let me get that example up here. So let's say I have this hypothetical search tree right? I want to implement my search in such a fashion that this hypothetical search tree gets explored deep as node first. Right? Let's say M is my solution for argument's sake. So to do that, I will implement my fringe as a. Last and first out data structure a stack. Right? Initially my friends will have the node a I will pop that. Node A is not my goal. So I will generate its successors. And once I generated the successors, I'll push them into the fringe again. I can push B followed by C, or I can push C, followed by B. Let's say I did the latter. Pop. The node at the front of the fringe is B my goal? No, it is not. So generate its successors, push them into the fringe. Let us say I pushed E followed by D pop. The node in the front of the fringe is D my goal? No, it is not. So generate its successors. Push the nodes into the fringe. Part the node from the front of the fringe. Generate its successors. Now, according to my successor function, H does not have any successors. Is there a point in storing it in memory anymore? No, it is not the goal. And since it does not have any successes. It is not going to be on any path to the goal. So there really is no point in storing it in memory anymore. So I can discard that node and recover that memory. Now note this is an uh, like this does require a little bit of overhead. You need to write code that will discard the mode, recover, discard the node and recover the memory, etc., etc. but it can be done. And if you did do it, you can discard that node and recover that memory that was used for the next node. Is I my goal node is not generate I successors. I does not have any successes. So now you know that I is not the goal. I is not going to be on any path to the goal so I can discard I from memory. And because I discarded both H and I, I now also know that D is not going to be on the goal side, is not going to be on any path to the goal. So I can discard the also. Right. So that means I can discard all of these nodes and recover that memory. Then pop the node at the front of the fringe is E my goal? No it is not. So generate h successors, which is going to be. These push them into the fringe of the node at the front of the fringe is my goal. No, it is not. Is general J. Successor J does not have any successors, so I can discard j from memory of the node at the front of the fringe is my goal. No, it is not, so generate k successor K does not have any successors, so I can discard k from memory. Because I discarded both j and k, I can discard e from memory, and because I discarded both d and e, I can discard B from memory. So now this entire subtree is worth of memory. I can recover right from the node at the front of the fringe is C see my goal? No, it is not. So generate its successes. Push them into the. Fringe. Pop the node from the front of the fringe is f my goal. No, it is not. So generate h successors push them to the fringe of the node. At the front of the fringe. Is L my goal? No, it is not generated. Successors L does not have any successors, so I can discard it from memory. Pop. The node at the front of the fringe is M my goal? Yes, yes it is. So generate the solution and I am done. Right. So this is depth first search. Now is it complete? If a solution exists are you guaranteed to find it. Yes. Okay. Let us go back to this problem. Again. My fringe is a stack. Pop the node A is a my goal. No, it is not. So generate its successors. Push them into the fringe. Of the node at the front of the fringe is be my goal. No, it is not. So generate its successors. Push them into the front of the fringe. Pop the node at the front of the fringe is a my goal. No, it is not. So generate its successors. Push them into the front of the fringe. Pop the node at the front of the fringe is be my goal. No, it is not. So generate its successors. You guys kind of get the idea of where this is going. So if M is infinity, that is if you have an infinite length search tree then DFS is not complete. Now, if the reason your search tree is of infinite length is because of limited states, then you can make DFS complete by using graph search. But if it is just inherently infinite, like say for example, you are dealing with a problem where there an infinite number of possible states that can exist, which is a possibility. Then even using graph search does not guarantee that DFS will find a solution, right? So DFS will find a solution for finite length spaces. Otherwise it is not complete. So in general it is considered to be not complete because there is no guarantee that using graph search will make your search length finite. Right Time complexity. Worst case scenario, let's say I have a finite search tree. Worst case scenario. How many? How much of that search tree will I end up exploring before I find a solution for all of it? So worst case, time complexity for DFS is B to the power of M, which is really, really bad because M is usually very large. But you need to take this with a grain of salt because this is a worst case scenario, right? Usually, especially if you are dealing with a problem where B is large, BFS will find a solution much faster than BFS. In practice, this is just the worst case scenario, right? What about space complexity? Well, every node that is not going to be on the path to the goal can be discarded. Once you realize that it's not going to be on the path to the goal. So in the worst case scenario, the only nodes that you'll end up storing are those nodes that are along the path to the goal. So that makes the space complexity of depth. First, search linear to em. Linear space complexity is very good compared to the exponential space complexity we had to do previous. Right. And the last question is it optimal? Well, the answer is no, right? BFS will find a shallower solution. UX will find the cheapest solution as long as M is finite, BFS will find a solution. That's the only guarantee. It's not necessarily going to be the shallowest, not necessarily going to be the cheapest. It will be a solution like for this problem. It is entirely possible for BFS DFS to have given the solution A to B, to A to E to D. Which is our possible solution. And DFS could have found that solution, right? There is no guarantee that it will. By the way this is a common misconception. If a search strategy is not complete, it does not mean that it will never find a solution. It just means that you can't guarantee it. Right? Okay. Now we will get to this issue later. But first let's deal with that issue. The fact that it is not complete. Now if M is infinity, because there are a lot of repeated a stage. We already know how to deal with it. Do graph search instead of tree search. But let's say I have a problem where m is infinity. Even though there are no repeated states, there are just an infinite number of states, so the search tree will stretch off to infinity. I still want to do DFS and I want my algorithm to complete that is finished executing at some point. What can I do? What modification can I take this question? I have an infinite length search tree, not because of repeated steps, just because there are an infinite number of states. I want to do a depth first traversal on that tree, and I want my traversal to finish at some point. Whether or not it finds the node I am looking for is irrelevant. I want it to finish. What modification can I do? This. There are no revisited nodes. Yeah. You will add a depth limit. You will say that. Explore the tree until you reach this limit. Then start back. Right. Or to put it another way, you will modify your expression such that any node at depth level L does not have any successors, so that is known as depth limited search, right? All you have to do to achieve it is very simple. You will just have one extra piece of kit here. So you will have this. So you will implement this pseudocode where this line will be implementing the fringe as a stack. In addition, after this line you will do one additional check. You will check is the depth of the node L where L is some predetermined number. If it is, Don't generate any successes. Otherwise, generate successes and add them to the fridge. Right, so you are effectively cutting off your search tree at level L l is the new M right now? Is such a method complete? Will it find a solution if one exists? Yes.

SPEAKER 6
If one existed.

SPEAKER 7
Before the level. Yeah.

SPEAKER 2
So as long as L is large enough, right. If the first reachable solution occurs at depth 30, let's say, and you set your depth limit to be 25, it's not going to find a solution. It will complete but it won't find a solution. Sorry I shouldn't say complete. The execution will finish, but it won't find a solution, right? But if I set the depth limit to be any number greater than 30, then there is at least one solution it could possibly reach. So it may find a solution, right? So or to put it more formally. As long as. It is complete. Now the problem is you don't know what D is. So I did making sure that L is larger than that is kind of tricky to do when you don't know what it is. Now you don't also want to set D to be some really, really large number. Because if L is the new M, then what happens to the worst case time complexity of your depth limited search. Right. So if you set L to be really, really large you will find a solution. But you may end up doing an extreme amount of work before you find that solution, right? Thankfully, this does not translate to bad memory complexity because memory complexity is still. Now, the last question. Is this optimal?

SPEAKER 0
Well, no.

SPEAKER 2
If there are multiple reachable solutions within the depth limit that you have set. It may find any one of them. Now, what would happen if L was exactly D? Then it will find the shallower solution, which if you are dealing with a problem where the shallower solution is the optimal solution, this would also be optimal, just like BFS would have been. Problem is, there is no way to know exactly what D is before you know what solutions are available, right? Unless you do something like. Oh come on. Iteratively deepening search. The idea behind this is fairly straightforward. I know that depth limited search will find a solution if L is large enough, but if L is too large, I may end up doing a unnecessary amount of work to find it. And if L is exactly D, then I will do just enough work to find a solution. And if I'm dealing with a problem where the shallower solution is also going to be the optimal solution, that solution is also going to be the optimal solution. So what I'll do is I'll set L to be zero run depth limited search. If my search fails, I'll set the deployment to be one try again. Set that limit to be two. Try again. Set depth limit to be three. Try again and slowly keep incrementing the depth limit till I reach a depth limit of T, in which case I will find the shallow solution. So this is the idea of being. Iteratively deepening search. So let's say we go back to the problem that we were discussing for depth first search. So I want to do iteratively deepening search for it. Initially my depth limit is zero. So my friends will have node A I will pop node A is not the goal, but the node is at depth level zero, so it will not generate any successors. So I will discard the node from memory. Next loop my friend is empty so my search has failed, right? By the way, there is a difference between failing because there is no solution and failing because I did not generate any successors. That's called cut off failure. So you will set a flag for it or something so that you know that, oh, there may still be a solution. I just couldn't find it. So once it gets that cut off message, it will increment the depth limit and try again, right? So set the depth limit to be one. Try again. Initialize your friends. Initially it will contain node A. Pop that node A is not my goal. A is not at the debt limit. So I will generate its successors, add them to the French, push them into the French. Pop the node at the front of the friends. Let's say its b, B is not the goal, B is at the limit, so no success is generated so it gets discarded from memory. Pop the node at the front of the friends c c is not the goal, C is at the diplomat, so no success has generated. So it gets discarded from memory because I discarded. Both B and C are also discarded from memory. Next loop fringe is now empty because I got cut off search. So return the message. Increment the depth limit. Try again. So I will set the depth limit to two. Try again. This iteration will also fail because these nodes will get cut off right? So increment the debt limit. Try again. And in this iteration, I will end up finding a node that satisfies the goal condition. So I will return it. So this is iteratively deepening search. Now is this complete. Yes yes yes because L is going to keep getting incremented incrementally till it reaches D. So that means if a solution is reachable then it will eventually reach it. So it will find a solution. The shallow a solution. What about time complexity. Well every node in level zero sorry. The node in level zero will get expanded in every iteration. The node all the nodes in level one in every iteration Except for the first one. All the nodes in level two in every iteration except the first and the second one, etc. etc. etc. and the nodes on level D will be expanded only once in the very last iteration, but their successors will not be generated, right? So that means that the time complexity as represented using Big-O notation is going to be Big-O of B to the power of D. Right now there is a little bit of redundant work involved here. Quite a bit of redundant work involved here, but that's fine, because every iteration at the end of every iteration, all the nodes that were explored in that iteration gets discarded from memory, which means that the only nodes that will stay in memory by the time you finish are the nodes along the path to the goal in that iteration alone, which means that the space complexity is linear to the. Right. And finally, is it optimal? Not in general, but if you are dealing with a problem where the shallower solution is guaranteed to be the optimal one, it will be optimal. It's going to be optimal for the same problems, but it would have been optimal for right. Now. One small note. Let's say I have a problem where the shallower solution is the optimal solution. It has a branching factor of ten and my solution occurred at depth level five. It is the very last node I would would have visited in depth level five. If I found that solution using BFS, this is the number of nodes that I would have ended up generating and storing in the fridge. Sorry. Generating and storing in memory. If I ended up doing IDs. This is the number of nodes I would have generated. Right now I don't need to store all of these nodes in memory. So memory complexity wise it has BFS beat but it looks like it has it beat even in time complexity wise, even though IDs is doing a lot of redundant work. Why is that? Well, that's because I will generate but not expand all the nodes in level six. Well, not all most of the nodes in level six. Is there some way in which I could avoid doing that work? If I want to do tree search with BFS, I would implement this pseudocode where this line is implemented as a first in, first out queue. What modification I can do here to avoid generating nodes that I will never end up expanding. That is, all those nodes on level T plus one. Should I call existing ones to get n inserting? Yeah. So move this test down here. Each time I generate a successor node, apply the goal, test it. So that way the moment a node that satisfies the gold test is generated, I will stop. So that returned my time complexity to order of B to the power of B rather than B to the power of D plus one. Which might not seem like it's a big difference, but it does tend to add up. Yeah, right now. Small note of caution though. You can't do this for uniform cost search. You can only do this for BFS because uniform cost search you can generate a suboptimal go long before you generate an optimal code. So the only guarantee you can give is you will never expand a suboptimal goal before expanding the optimal goal. So if you do this modification, you are no longer guaranteed optimality with UX. That doesn't mean there are no possible optimizations for UX. Let's say I have a problem where I have a. I have a large number of actions that will take me back to the same state. So I have what are called loops, a sequence of actions that will bring me back to a state. Right? Since there are a lot of repeated states, I would want to use uniform cost search. Sorry. Graph search. And since I'm interested in finding the optimal solution, I'm using uniform cost search. So I would be implementing this pseudocode right with this line implemented as a priority queue where the criteria for sorting is g of n Different value. Now I notice that my fringe ends up having a bunch of nodes that all represent the same state, right? I'm only going to expand the one with the lowest g of N, so is there any way I can make my fringe smaller? I could do the same thing as what closed set does. Only now, each time I generate a successor node, I will compare it to all the nodes currently in the fringe. If there is another node that represents the same state already in there, I will compare that g of and values. Whichever one has the lowest g of n value goes into the fringe, the other one gets thrown away. Right now that's a lot of additional overhead. I need to scan through the entire fringe. I need to add go to remove from the middle of the fringe, etc., etc. but that can come back my friends, which can end up saving time. If you are dealing with problems where you have a large number of search candidates that all represent the same state being generated. Like for example, a graph navigation problem. Anybody want to take a guess what that algorithms call? It's optimized for graph navigation problems. That should be a clue. It's Dijkstra's algorithm. That's pretty much what Dijkstra's is. Dijkstra's is nothing more than a modified version of, uh, graph search with uniform cost search. And we are discussing the general method of building a goal based agent. There are problems specific, task specific, or even strategy specific optimizations and modifications you can do. In fact, some of the later optimized algorithms are basically this algorithm with a few extra modifications done for that type of problem, right? Like when we look at CSPs, we'll be looking at an algorithm that is little more than three. Search with depth, first search strategy that has been modified to work better with that type of problem, etc.. Right. So you can have those modifications available later. This is the generic approach, right? And these strategies are the uninformed search strategies that you can use. Right now. Of these search strategies only uniform cost search is optimal in general. Now if you're dealing with a problem where the shallow solution is going to be the optimal solution, BFS and IDs are also options, right? All of them have the potential of being extremely inefficient as far as time complexity is concerned. They're all exponential time complexity exponential to either the depth of the solution or the cost of the solution, or the length of the entire search tree.

SPEAKER 8
Right?

SPEAKER 2
But DFS and its derivatives will have at least linear memory complexity. You don't necessarily have to store every single node in memory. Now, regarding completeness, BFS is assumed to be complete in general because its only requirement is B has to be finite, which is always the case most of the time, right? UX is complete only as long as action cost step costs are non-zero, which is very easy to enforce, but it is something you have to do right? And IDs is considered to be complete. BFS and depth limited search are not complete in general, because DFS is not complete unless your search tree is finite and DLS is not complete unless L is large enough right now. I would love to get started on, uh, informed search now, but I don't think I have enough time to do so. Or class ends at 650, so we will pick up with that during Thursday's class. All right. If you guys have any questions, I'd be happy to answer them. If not, I'll see you guys on Thursday.

SPEAKER 7
I can't. So I just wanted a clarity on the notations. Yeah. P stands for the branching factor.

SPEAKER 2
Not with its maximum number of actions that can be taken in any given state okay.

SPEAKER 7
This stands for depth.

SPEAKER 2
The depth at which the first reachable solution occurs, or.

SPEAKER 7
The solution.

SPEAKER 2
That takes.

SPEAKER 7
The minimum or the minimum.

SPEAKER 2
Number of actions required to get to a solution.

SPEAKER 0
Yeah.

SPEAKER 7
So in USA UX, right. Uh, we're in a branch here with a cost of seven. So how this will be the optimal.

SPEAKER 9
What do you mean?

SPEAKER 7
Uh, we see the cheapest right solution in the UX, right? Yeah. So if we have a branch over here from B to F, of course. Seven. So how will it be the optimal.

SPEAKER 2
Because from where to where. What's your start a to D okay. So what's your question.

SPEAKER 7
If your branch would be to be to F it's cos seven. Yeah. So everything will be six six and from B it will be 13. Yeah. And if from B to A another source successor will be the 12.

SPEAKER 2
Cheapest cheapest way to get to D.

SPEAKER 7
Yeah.

SPEAKER 2
Okay. I'm not understanding. What do you mean. Why will it be optimal.

SPEAKER 7
How it will be optimal. Because as we go to A to B c our cost will be six and then our B is not our goal. Right.

SPEAKER 2
Yeah. So you'll generate its successors which is going to be uh, so you when you expand a you will generate two successors B six and E eight. Yeah. Right. Then you will expand this. That will generate a 12. and, uh, f 13. Yeah. Right. Then you'll expand this? No, no. Oh, yeah. So you'll also just select C 16. Then you will expand E eight. Right. And you will generate a 16. And uh let's see uh eight plus five is uh 1313. So f 13 which will be here. Yeah. Right. Then you will expand a 12 right. Which will generate. Let's see uh P 18 and E 20. Then you will expand F 13. Right. Oh sorry this was B 13. Yeah.

SPEAKER 7
No it will be.

SPEAKER 2
F 13 F13. You will expand F 13 which will generate B 20. Uh then E 18 which will go here and uh D 28 which will go here. Then you'll expand F13, right? Oh, sorry. This was.

SPEAKER 7
This was a.

SPEAKER 2
E 13. No no no, this was this f 13. Yeah. This f 13, which will again generate three copies of these nodes and add them to the French. Then you will expand C uh 16 which will generate. Let's see uh D 17 which will go here and B 20 which will go, you know, uh B 26 which will go here. Then you will expand a 16, generate its success and add them. Eventually you will get to this D uh 17 okay. Right. So that is the optimal path.

SPEAKER 7
Answer one more question that how can we estimate def limit as in dl as DLLs. You do not.

SPEAKER 2
Estimate it you give a depth limit. So that is the problem with depth limited search whether or not it finds a solution, how much work it does. All of it depends on what debt limit you give it, which is why we have iteratively deepening search, which removes the problem of coming up with a good debt limit from us. Okay.

SPEAKER 6
Thank you sir. Yeah. The notations. Yeah. The balancing.

SPEAKER 7
Factor D is the minimum number of actions required for reaching the shallowest.

SPEAKER 2
Solution. And Ellis. Ellis, what debt limit you have set when you are doing deep limited search whatever you want to set. How do you know M is the max length of the search tree? If you were to generate the entire search tree, how big can it get? Which is a number that can be infinity? Yeah. And G of n is the cumulative cost of all the actions required to get to a node from the initial state. Okay.

SPEAKER 6
Yeah.

SPEAKER 0
So I do general doubt. So I'm a job.

SPEAKER 7
Programmer.

SPEAKER 0
Okay I.

SPEAKER 7
Will do all the.

SPEAKER 0
Assignments in Java. But as.
